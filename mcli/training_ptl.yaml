name: hpa-subcellular
image: ankitgpt/pytorch-lightning-vision:latest # pytorchlightning/pytorch_lightning:base-cuda-py3.11-torch2.3-cuda12.1.0
compute:
  gpus: 16
  cluster: r18z1p1
integrations:
- integration_type: git_repo
  git_repo: aktgpt/sub-cell-embed
env_variables:
  MCLI_TIMEOUT: 3600
command: |-
  # conda install -y nvidia/label/cuda-12.1.1::cuda-compiler
  # pip install deepspeed kornia
  # ds_report
  pip install --upgrade pip transformers

  aws s3 cp s3://ai-residency-stanford-subcellgenai/hpa_mds_448/valid/index.json .
  aws s3 cp s3://ai-residency-stanford-subcellgenai/hpa_mds_448/train/index.json .
  
  cd sub-cell-embed
  
  # aws s3 cp --recursive s3://ai-residency-stanford-subcellgenai/allhpa_outputs/rybg_allhpa_nc16_contrastmaepool_byol_vitb16_ptl_masknorm/ .local/allhpa_mosaic/rybg_allhpa_nc16_contrastmaepool_byol_vitb16_ptl_masknorm/
  # ankitgpt/pytorch-lightning-vision:latest

  # sed -i 's/UNIQUE_CATS.append("Negative")/#UNIQUE_CATS.append("Negative")/g' data/dataset2.py

  composer main_lightning.py --config /mnt/config/parameters.yaml 

  aws s3 sync /mnt/workdisk/subcell-embed/exps/allhpa_ablations/ s3://ai-residency-stanford-subcellgenai/allhpa_ablations/ 
parameters:
  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_vitb16_fp32_supervised" 
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: False
  # model:
  #   vit_model:
  #     name: "ViTModel"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: true
  #   pl_args:
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "BaseSupervised"
  #     train_batch_size: 64
  #     test_batch_size: 64
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision: "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"



  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_contrast_mae_pool_vitl16_fp32_mr0.25_objmr0.0_byol" #  
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: True
  # model:
  #   mae_model:
  #     name: "ViTMAEForPreTraining"
  #     args:
  #       hidden_size: 1024
  #       num_hidden_layers: 24
  #       num_attention_heads: 16
  #       intermediate_size: 4096
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.1
  #       attention_probs_dropout_prob: 0.1
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: True
  #       decoder_num_attention_heads: 16
  #       decoder_hidden_size: 512
  #       decoder_num_hidden_layers: 8
  #       decoder_intermediate_size: 2048
  #       mask_ratio: 0.25
  #       norm_pix_loss: True
  #       object_mask_ratio: 0.0
  #   ssl_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 2048
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   supcon_model:
  #     name: CentroidDiffusionEMALoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 512
  #           mlp_layers:
  #             - 8192
  #             - 512
  #       labd_brownian: 0.0
  #       n_views: 8
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 1024
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_recon: 0.5
  #     weight_ssl: 0.5
  #     weight_supcon: 1.0
  #     momentum: 0.995
  #     mask_ratio2: 0.05
  #     max_epochs: 300
  #     init_lr: 1.e-5
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "ContrastBYOLMAE"
  #     train_batch_size: 112
  #     test_batch_size: 112
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"




  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_contrast_mae_pool_vitb16_fp32_mr0.25_objmr0.0_byol_lambd0" #   
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: True
  # model:
  #   mae_model:
  #     name: "ViTMAEForPreTraining"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: True
  #       decoder_num_attention_heads: 16
  #       decoder_hidden_size: 512
  #       decoder_num_hidden_layers: 8
  #       decoder_intermediate_size: 2048
  #       mask_ratio: 0.25
  #       norm_pix_loss: True
  #       object_mask_ratio: 0.0
  #   ssl_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 1536
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   supcon_model:
  #     name: CentroidDiffusionEMALoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 512
  #           mlp_layers:
  #             - 8192
  #             - 512
  #       labd_brownian: 0.0
  #       n_views: 8
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 768
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_recon: 0.5
  #     weight_ssl: 0.5
  #     weight_supcon: 1.0
  #     momentum: 0.995
  #     mask_ratio2: 0.0
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "ContrastBYOLMAE"
  #     train_batch_size: 112
  #     test_batch_size: 112
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"



  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_contrast_mae_pool_vitl16_fp32_mr0.25_objmr0.0_supcon2" #
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: True
  # model:
  #   mae_model:
  #     name: "ViTMAEForPreTraining"
  #     args:
  #       hidden_size: 1024
  #       num_hidden_layers: 24
  #       num_attention_heads: 16
  #       intermediate_size: 4096
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: True
  #       decoder_num_attention_heads: 16
  #       decoder_hidden_size: 512
  #       decoder_num_hidden_layers: 8
  #       decoder_intermediate_size: 2048
  #       mask_ratio: 0.25
  #       norm_pix_loss: True
  #       object_mask_ratio: 0.0
  #   ssl_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 2048
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   supcon_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 2048
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 1024
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_recon: 1.0
  #     weight_ssl: 1.0
  #     weight_supcon: 0.1
  #     mask_ratio2: 0.0
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "ContrastMAE"
  #     train_batch_size: 144
  #     test_batch_size: 144
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"


  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_contrast_mae_pool_vitb8_fp32_mr0.25_objmr0.0_supcon" #  
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: True
  # model:
  #   mae_model:
  #     name: "ViTMAEForPreTraining"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 8
  #       num_channels: 4
  #       qkv_bias: True
  #       decoder_num_attention_heads: 16
  #       decoder_hidden_size: 512
  #       decoder_num_hidden_layers: 8
  #       decoder_intermediate_size: 2048
  #       mask_ratio: 0.25
  #       norm_pix_loss: True
  #       object_mask_ratio: 0.0
  #   ssl_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 1536
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   supcon_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 1536
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 768
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_recon: 1.0
  #     weight_ssl: 1.0
  #     weight_supcon: 0.1
  #     mask_ratio2: 0.1
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "ContrastMAE"
  #     train_batch_size: 128
  #     test_batch_size: 128
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed"  #
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"


  exp_folder: "/mnt/workdisk/subcell-embed/exps"
  exp_name: "allhpa_ablations"
  exp_mode: "rybg_448_nc8_contrast_mae_vitb16_fp32_mr0.33_objmr0.0" #  _pool _supcon rybg_448_nc8_contrast_mae_vitb16_fp32_mr0.25_objmr0.0
  log_wandb: true
  num_workers: 16
  pin_memory: false
  data:
    dataset: "HPASubCellDataset"
    image_size: 448
    train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
    val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
    test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
    args:
      ssl_transform: true
      protein_path: null 
      n_cells: 8
      mask_prob: 0.5
      color_channels:
        - "red"
        - "yellow"
        - "blue"
        - "green"
      normalize: "min_max"
      return_cell_mask: True
  model:
    mae_model:
      name: "ViTMAEForPreTraining"
      args:
        hidden_size: 768
        num_hidden_layers: 12
        num_attention_heads: 12
        intermediate_size: 3072
        hidden_act: "gelu"
        hidden_dropout_prob: 0.0
        attention_probs_dropout_prob: 0.0
        initializer_range: 0.02
        layer_norm_eps: 1.e-12
        image_size: 448
        patch_size: 16
        num_channels: 4
        qkv_bias: True
        decoder_num_attention_heads: 16
        decoder_hidden_size: 512
        decoder_num_hidden_layers: 8
        decoder_intermediate_size: 2048
        mask_ratio: 0.33
        norm_pix_loss: True
        object_mask_ratio: 0.0
    ssl_model:
      name: ContrastiveLoss
      args:
        projector:
          name: "ProjectionHead"
          args:
            in_channels: 768
            mlp_layers:
              - 8192
              - 8192
              - 512
        temperature: 0.1
    # supcon_model:
    #   name: ContrastiveLoss
    #   args:
    #     projector:
    #       name: "ProjectionHead"
    #       args:
    #         in_channels: 768
    #         mlp_layers:
    #           - 8192
    #           - 8192
    #           - 512
    #     temperature: 0.1
    # pool_model:
    #   name:  "GatedAttentionPooler"
    #   args:
    #     dim: 768
    #     int_dim: 512
    #     num_heads: 2
    #     dropout: 0.2
    pl_args:
      weight_recon: 1.0
      weight_ssl: 1.0
      weight_supcon: 0.1
      mask_ratio2: 0.0
      max_epochs: 300
      init_lr: 1.e-4
      weight_decay: 0.05
      betas:
        - 0.9
        - 0.95
      warmup_epochs: 5
  train:
      pl_module: "ContrastMAE"
      train_batch_size: 112
      test_batch_size: 112
      ckpt_path: "last.ckpt"
  trainer:
      gc_interval: 50
      valid_every: 10
      strategy: "ddp"
      logging_interval: 50
      checkpoint_interval: 50
      precision:  "32-true" # "bf16-mixed" # 
  test:
    tester: "MAETester"
    model_path: "best_model.ckpt"



  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "bg_448_nc8_vitl16_pool_fp32_supcon3" #contrast_
  # log_wandb: true
  # num_workers: 24
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       # - "red"
  #       # - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: False
  # model:
  #   vit_model:
  #     name: "ViTModel"
  #     args:
  #       hidden_size: 1024
  #       num_hidden_layers: 24
  #       num_attention_heads: 16
  #       intermediate_size: 4096
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 2
  #       qkv_bias: True
  #   # ssl_model:
  #   #   name: ContrastiveLoss
  #   #   args:
  #   #     projector:
  #   #       name: "ProjectionHead"
  #   #       args:
  #   #         in_channels: 768
  #   #         mlp_layers:
  #   #           - 8192
  #   #           - 8192
  #   #           - 512
  #   #     temperature: 0.1
  #   supcon_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 2048
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 1024
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_ssl: 1.0
  #     weight_supcon: 1.0
  #     max_epochs: 300
  #     init_lr: 5.e-5
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "BaseSSL"
  #     train_batch_size: 144
  #     test_batch_size: 144
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision: "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"


  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "bg_448_nc8_vitb8_pool_fp32_supcon" #contrast_
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       # - "red"
  #       # - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: False
  # model:
  #   vit_model:
  #     name: "ViTModel"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 8
  #       num_channels: 2
  #       qkv_bias: true
  #   # ssl_model:
  #   #   name: ContrastiveLoss
  #   #   args:
  #   #     projector:
  #   #       name: "ProjectionHead"
  #   #       args:
  #   #         in_channels: 768
  #   #         mlp_layers:
  #   #           - 8192
  #   #           - 8192
  #   #           - 512
  #   #     temperature: 0.1
  #   supcon_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 1536
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   pool_model:
  #     name:  "GatedAttentionPooler"
  #     args:
  #       dim: 768
  #       int_dim: 512
  #       num_heads: 2
  #       dropout: 0.2
  #   pl_args:
  #     weight_ssl: 1.0
  #     weight_supcon: 1.0
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "BaseSSL"
  #     train_batch_size: 128
  #     test_batch_size: 128
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision: "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"


  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_contrast_vitb16_fp32_byol_mom0.99" # 
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: False
  # model:
  #   vit_model:
  #     name: "ViTModel"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: true
  #   ssl_model:
  #     name: ContrastiveLoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 768
  #           mlp_layers:
  #             - 8192
  #             - 8192
  #             - 512
  #       temperature: 0.1
  #   supcon_model:
  #     name: CentroidDiffusionEMALoss
  #     args:
  #       projector:
  #         name: "ProjectionHead"
  #         args:
  #           in_channels: 512
  #           mlp_layers:
  #             - 8192
  #             - 512
  #       labd_brownian: 0.25
  #       n_views: 8
  #   pl_args:
  #     momentum: 0.99
  #     weight_ssl: 0.5
  #     weight_supcon: 1.0
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "BYOL_SSL"
  #     train_batch_size: 112
  #     test_batch_size: 112
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"









  # exp_folder: "/mnt/workdisk/subcell-embed/exps"
  # exp_name: "allhpa_ablations"
  # exp_mode: "rybg_448_nc8_mae_vitb16_fp32_mr0.5_objmr0.5"
  # log_wandb: true
  # num_workers: 16
  # pin_memory: false
  # data:
  #   dataset: "HPASubCellDataset"
  #   image_size: 448
  #   train_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/train # "s3://ai-residency-stanford-subcellgenai/hpa_mds/train/"
  #   val_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/valid # "s3://ai-residency-stanford-subcellgenai/hpa_mds/valid/"
  #   test_remote_path: s3://ai-residency-stanford-subcellgenai/hpa_mds/test #"s3://ai-residency-stanford-subcellgenai/hpa_mds/test/"
  #   args:
  #     ssl_transform: true
  #     protein_path: null 
  #     n_cells: 8
  #     mask_prob: 0.5
  #     color_channels:
  #       - "red"
  #       - "yellow"
  #       - "blue"
  #       - "green"
  #     normalize: "min_max"
  #     return_cell_mask: True
  # model:
  #   mae_model:
  #     name: "ViTMAEForPreTraining"
  #     args:
  #       hidden_size: 768
  #       num_hidden_layers: 12
  #       num_attention_heads: 12
  #       intermediate_size: 3072
  #       hidden_act: "gelu"
  #       hidden_dropout_prob: 0.0
  #       attention_probs_dropout_prob: 0.0
  #       initializer_range: 0.02
  #       layer_norm_eps: 1.e-12
  #       image_size: 448
  #       patch_size: 16
  #       num_channels: 4
  #       qkv_bias: True
  #       decoder_num_attention_heads: 16
  #       decoder_hidden_size: 512
  #       decoder_num_hidden_layers: 8
  #       decoder_intermediate_size: 2048
  #       mask_ratio: 0.5
  #       norm_pix_loss: True
  #       object_mask_ratio: 0.5
  #   pl_args:
  #     max_epochs: 300
  #     init_lr: 1.e-4
  #     weight_decay: 0.05
  #     betas:
  #       - 0.9
  #       - 0.95
  #     warmup_epochs: 5
  # train:
  #     pl_module: "BaseMAE"
  #     train_batch_size: 112
  #     test_batch_size: 112
  #     ckpt_path: "last.ckpt"
  # trainer:
  #     gc_interval: 50
  #     valid_every: 10
  #     strategy: "ddp"
  #     logging_interval: 50
  #     checkpoint_interval: 50
  #     precision:  "32-true" # "bf16-mixed" # 
  # test:
  #   tester: "MAETester"
  #   model_path: "best_model.ckpt"







